{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This notebook has the results of the preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import geopandas\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202111 is not a subset\n",
      "202112 is not a subset\n"
     ]
    }
   ],
   "source": [
    "#we first need to load the data from ./data\n",
    "#assignment 1 focusses on the sales, so let's import that data into a pandas dataframe\n",
    "\n",
    "append_string_list1 = [\"202106\",\"202107\",\"202108\",\"202109\",\"202110\",\"202111\",\"202112\"]#the names we try to process\n",
    "append_string_list2 = []#the leftover names\n",
    "final_df_list = []\n",
    "needed_columns = ['Transaction Date','Transaction Type','Product id','Sku Id','Buyer Country','Buyer Postal Code','Amount (Merchant Currency)']\n",
    "needed_columns_lower = [col.lower() for col in needed_columns]#the lower case names\n",
    "needed_columns_set = set(needed_columns_lower)#a set of this list, to check with subset later\n",
    "\n",
    "for name in append_string_list1:\n",
    "    if not final_df_list:#we have the first df\n",
    "        first_df = pd.read_csv(f\"./data/sales_{name}.csv\")\n",
    "        first_df.columns = first_df.columns.str.lower()#always lower case column names\n",
    "        final_df_list.append(first_df[needed_columns_lower])\n",
    "    else:\n",
    "        #we already have the first dataframe, now we need to check for every column type if it's in the columns list\n",
    "        next_df = pd.read_csv(f\"./data/sales_{name}.csv\")\n",
    "        next_df.columns = next_df.columns.str.lower()#convert to lower case\n",
    "        columns_set = set(next_df.columns)\n",
    "        if needed_columns_set.issubset(columns_set):\n",
    "            final_df_list.append(next_df[needed_columns_lower])\n",
    "        else:\n",
    "            append_string_list2.append(name)\n",
    "            print(f\"{name} is not a subset\")\n",
    "\n",
    "#let's do some data type processing on the list we already have\n",
    "for i in range(len(final_df_list)):\n",
    "    final_df_list[i].loc[:,'transaction date'] = pd.to_datetime(final_df_list[i]['transaction date']).dt.date#convert to datetime\n",
    "    final_df_list[i] = final_df_list[i].loc[final_df_list[i]['transaction type'] == \"Charge\"] #drop columns that aren't charge\n",
    "    final_df_list[i] = final_df_list[i].loc[final_df_list[i]['product id'] == \"com.vansteinengroentjes.apps.ddfive\"] #only use this product id\n",
    "\n",
    "\n",
    "pd.concat(final_df_list).to_csv(\"finaldf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we need to process the append_string_list2. The data in here has a different structure than our other data\n",
    "#Because of this, we will construct these from the ground up, and we will zoom in on just these datasets (so without a loop)\n",
    "df1 = pd.read_csv(f\"./data/sales_{append_string_list2[0]}.csv\")\n",
    "df2 = pd.read_csv(f\"./data/sales_{append_string_list2[1]}.csv\")\n",
    "#let's check if the column names are the same\n",
    "df1.columns = df1.columns.str.lower()#convert to lower case\n",
    "df2.columns = df1.columns.str.lower()#convert to lower case\n",
    "columns_set1 = set(df1.columns)\n",
    "columns_set2 = set(df2.columns)\n",
    "print()\n",
    "\n",
    "if columns_set1.issubset(columns_set2) or columns_set2.issubset(columns_set1):\n",
    "#we can append to the list\n",
    "    last_df = pd.concat([df1,df2])\n",
    "else:\n",
    "    print(\"not a subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These last 2 have the same structure, we only need to create a dataframe that matches the first one\n",
    "#We go column by column to see if it matches\n",
    "\n",
    "#Transaction date column is called 'order charged timestamp', let's change that\n",
    "last_df.rename(columns={\"order charged date\" : \"transaction date\"}, inplace=True)\n",
    "#this column also needs to be changed to the datetime format\n",
    "last_df.loc[:,'transaction date'] = pd.to_datetime(last_df['transaction date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction type is now called financial status:\n",
    "last_df.rename(columns={\"financial status\" : \"transaction type\"}, inplace=True)\n",
    "\n",
    "#but also the data type is different. Instead of being \"Charge\" and \"Google Fee\" we now have \"Charged\" and \"Refund\".\n",
    "#Since we end up only using the \"Charge\" entries anyways we will change the name \"Charged\" to \"Charge\" and we leave out \"Refund\".\n",
    "last_df = last_df[last_df[\"transaction type\"] == \"Charged\"]\n",
    "last_df[\"transaction type\"] = last_df[\"transaction type\"].replace('Charged', 'Charge', regex=False)\n",
    "\n",
    "#product id and sku id are the same, so we only have to filter on the product id:\n",
    "last_df = last_df.loc[last_df['product id'] == \"com.vansteinengroentjes.apps.ddfive\"] #only use this product id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Country of Buyer needs to be renamed to \"Buyer Country\"\n",
    "last_df.rename(columns={\"country of buyer\" : \"buyer country\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202111 is not a subset\n",
      "202112 is not a subset\n"
     ]
    }
   ],
   "source": [
    "#we first need to load the data from ./data\n",
    "#assignment 1 focusses on the sales, so let's import that data into a pandas dataframe\n",
    "\n",
    "append_string_list1 = [\"202106\",\"202107\",\"202108\",\"202109\",\"202110\",\"202111\",\"202112\"]#the names we try to process\n",
    "append_string_list2 = []#the leftover names\n",
    "final_df_list = []\n",
    "needed_columns = ['Transaction Date','Transaction Type','Product id','Sku Id','Buyer Country','Buyer Postal Code','Amount (Merchant Currency)']\n",
    "needed_columns_lower = [col.lower() for col in needed_columns]#the lower case names\n",
    "needed_columns_set = set(needed_columns_lower)#a set of this list, to check with subset later\n",
    "\n",
    "for name in append_string_list1:\n",
    "    if not final_df_list:#we have the first df\n",
    "        first_df = pd.read_csv(f\"./data/sales_{name}.csv\")\n",
    "        first_df.columns = first_df.columns.str.lower()#always lower case column names\n",
    "        final_df_list.append(first_df[needed_columns_lower])\n",
    "    else:\n",
    "        #we already have the first dataframe, now we need to check for every column type if it's in the columns list\n",
    "        next_df = pd.read_csv(f\"./data/sales_{name}.csv\")\n",
    "        next_df.columns = next_df.columns.str.lower()#convert to lower case\n",
    "        columns_set = set(next_df.columns)\n",
    "        if needed_columns_set.issubset(columns_set):\n",
    "            final_df_list.append(next_df[needed_columns_lower])\n",
    "        else:\n",
    "            append_string_list2.append(name)\n",
    "            print(f\"{name} is not a subset\")\n",
    "\n",
    "#let's do some data type processing on the list we already have\n",
    "for i in range(len(final_df_list)):\n",
    "    final_df_list[i].loc[:,'transaction date'] = pd.to_datetime(final_df_list[i]['transaction date']).dt.date#convert to datetime\n",
    "    final_df_list[i] = final_df_list[i].loc[final_df_list[i]['transaction type'] == \"Charge\"] #drop columns that aren't charge\n",
    "    final_df_list[i] = final_df_list[i].loc[final_df_list[i]['product id'] == \"com.vansteinengroentjes.apps.ddfive\"] #only use this product id\n",
    "\n",
    "\n",
    "pd.concat(final_df_list).to_csv(\"finaldf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"postal code of buyer\" needs to be renamed to \"buyer postal code\"\n",
    "last_df.rename(columns={\"postal code of buyer\" : \"buyer postal code\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we lost 3 rows of data\n"
     ]
    }
   ],
   "source": [
    "#We need to change \"charged amount\" to \"amount (merchant currency)\". But we also need to convert the prices\n",
    "#to euros. We assume in the original data that the amount (merchant currency) is in euros, per the assignment.\n",
    "#so we need to convert these values to euros using the conversion of currencies to euros.\n",
    "from currency_converter import CurrencyConverter\n",
    "c = CurrencyConverter(fallback_on_wrong_date=True, fallback_on_missing_rate=True)\n",
    "#we will this currencyconverter. However, not all currencies are supported, the ones that aren't we will have to\n",
    "#remove from the dataset. We can check this with the following set:\n",
    "old_length = len(last_df)\n",
    "last_df = last_df[last_df['currency of sale'].isin(c.currencies)].copy()\n",
    "print(f\"we lost {old_length-len(last_df)} rows of data\")\n",
    "last_df['amount (merchant currency)'] = last_df.apply( lambda row: c.convert(row['charged amount'],row[\"currency of sale\"], date=row[\"transaction date\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, let's filter on our used columns and append the prior dataframe to it to get the full dataframe\n",
    "final_df1 = pd.concat(final_df_list,ignore_index=True) #we concatinate the entire list to one dataframe\n",
    "final_df2 = last_df[needed_columns_lower]\n",
    "\n",
    "sales_df = pd.concat([final_df1, final_df2],ignore_index=True)\n",
    "sales_df.to_csv(\"sales_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASC_ass1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
